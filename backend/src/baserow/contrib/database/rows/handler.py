import re
from collections import defaultdict
from copy import copy
from decimal import Decimal
from math import ceil
from typing import Any, Dict, List, NewType, Optional, Set, Tuple, Type, cast

from django.contrib.auth.models import AbstractUser
from django.core.exceptions import ValidationError
from django.db import transaction
from django.db.models import Max, Q, QuerySet
from django.db.models.fields.related import ForeignKey, ManyToManyField
from django.utils.encoding import force_str

from baserow.contrib.database.fields.dependencies.handler import FieldDependencyHandler
from baserow.contrib.database.fields.dependencies.update_collector import (
    FieldUpdateCollector,
)
from baserow.contrib.database.fields.field_cache import FieldCache
from baserow.contrib.database.fields.field_filters import (
    FILTER_TYPE_OR,
    AnnotatedQ,
    FilterBuilder,
)
from baserow.contrib.database.fields.models import LinkRowField
from baserow.contrib.database.fields.registries import FieldType, field_type_registry
from baserow.contrib.database.table.models import GeneratedTableModel, Table
from baserow.contrib.database.table.operations import (
    CreateRowDatabaseTableOperationType,
    ImportRowsDatabaseTableOperationType,
)
from baserow.contrib.database.trash.models import TrashedRows
from baserow.core.handler import CoreHandler
from baserow.core.trash.handler import TrashHandler
from baserow.core.utils import Progress, get_non_unique_values, grouper

from .constants import ROW_IMPORT_CREATION, ROW_IMPORT_VALIDATION
from .error_report import RowErrorReport
from .exceptions import RowDoesNotExist, RowIdsNotUnique
from .operations import (
    DeleteDatabaseRowOperationType,
    MoveRowDatabaseRowOperationType,
    ReadDatabaseRowOperationType,
    UpdateDatabaseRowOperationType,
)
from .signals import (
    before_rows_delete,
    before_rows_update,
    rows_created,
    rows_deleted,
    rows_updated,
)
from .utils import find_intermediate_order

GeneratedTableModelForUpdate = NewType(
    "GeneratedTableModelForUpdate", GeneratedTableModel
)

RowsForUpdate = NewType("RowsForUpdate", QuerySet)


BATCH_SIZE = 1024


def serialize_errors_recursive(error):
    if isinstance(error, dict):
        return {
            key: serialize_errors_recursive(errors) for key, errors in error.items()
        }
    elif isinstance(error, list):
        return [serialize_errors_recursive(errors) for errors in error]
    else:
        if isinstance(error, ValidationError):
            return {"error": error.message, "code": error.code}
        if isinstance(error, Exception):
            return {"error": force_str(error), "code": "unknown_error"}
        return error


def prepare_field_errors(field_errors):
    """
    Here we update the index generated by the call of the create_rows method because
    that method received only valid rows so the index might be incorrect.
    """

    if not field_errors:
        return None

    return {
        field: serialize_errors_recursive(errs) for field, errs in field_errors.items()
    }


class RowHandler:
    def prepare_values(self, fields, values):
        """
        Prepares a set of values so that they can be created or updated in the database.
        It will check if the value can actually be set and prepares the value based on
        the field type.

        :param fields: The returned fields object from the get_model method.
        :type fields: dict
        :param values: The values that need to be prepared with the field id or the
            string 'field_{id}' as key.
        :type values: dict
        :return: The prepared values with the field name as key.
        :rtype: dict
        """

        return {
            field["name"]: field["type"].prepare_value_for_db(
                field["field"],
                values[field_id] if field_id in values else values[field["name"]],
            )
            for field_id, field in fields.items()
            if field_id in values or field["name"] in values
        }

    def prepare_rows_in_bulk(self, fields, rows, generate_error_report=False):
        """
        Prepares a set of values in bulk for all rows so that they can be created or
        updated in the database. It will check if the values can actually be set and
        prepares them based on their field type.

        :param fields: The returned fields object from the get_model method.
        :type fields: dict
        :param values: The rows and their values that need to be prepared.
        :type values: dict
        :return: The prepared values for all rows in the same structure as it was
            passed in.
        :rtype: dict
        """

        field_ids = {}
        prepared_values_by_field = defaultdict(dict)

        # organize values by field name
        for index, row in enumerate(rows):
            for field_id, field in fields.items():
                field_name = field["name"]
                field_ids[field_name] = field_id
                if field_name in row:
                    prepared_values_by_field[field_name][index] = row[field_name]

        # bulk-prepare values per field
        for field_name, batch_values in prepared_values_by_field.items():
            field = fields[field_ids[field_name]]
            field_type = field["type"]
            prepared_values_by_field[
                field_name
            ] = field_type.prepare_value_for_db_in_bulk(
                field["field"], batch_values, continue_on_error=generate_error_report
            )

        # replace original values to keep ordering
        prepared_rows = []
        failing_rows = {}
        for index, row in enumerate(rows):
            new_values = row
            row_errors = {}
            for field_id, field in fields.items():
                field_name = field["name"]
                if field_name in row:
                    prepared_value = prepared_values_by_field[field_name][index]
                    if isinstance(prepared_value, Exception):
                        row_errors[field_name] = [prepared_value]
                    else:
                        new_values[field_name] = prepared_value
            if not row_errors:
                prepared_rows.append(new_values)
            else:
                failing_rows[index] = row_errors

        return prepared_rows, failing_rows

    def extract_field_ids_from_keys(self, keys: List[str]) -> List[int]:
        """
        Extracts the field ids from a list of field names.
        For example keys like 'field_2', '3', 4 will be seen ass field ids.

        :param keys: The list of field names.
        :return: A list containing the field ids as integers.
        """

        field_pattern = re.compile("^field_([0-9]+)$")
        # @TODO improve this function
        return [
            int(re.sub("[^0-9]", "", str(key)))
            for key in keys
            if str(key).isnumeric() or field_pattern.match(str(key))
        ]

    def extract_field_ids_from_dict(self, values: Dict[str, Any]) -> List[int]:
        """
        Extracts the field ids from a dict containing the values that need to
        updated. For example keys like 'field_2', '3', 4 will be seen ass field ids.

        :param values: The values where to extract the fields ids from.
        :return: A list containing the field ids as integers.
        """

        return self.extract_field_ids_from_keys(values.keys())

    def get_internal_values_for_fields(
        self,
        row: GeneratedTableModel,
        fields_keys: List[str],
    ) -> Dict[str, Any]:
        """
        Gets the current values of the row before the update.

        :param row: The row instance.
        :param fields_keys: The fields keys that need to be exported.
        :return: The current values of the row before the update.
        """

        values = {}
        for field_id in self.extract_field_ids_from_keys(fields_keys):
            field = row._field_objects[field_id]
            field_type = field["type"]
            if field_type.read_only:
                continue
            field_name = f"field_{field_id}"
            field_value = field_type.get_internal_value_from_db(row, field_name)
            values[field_name] = field_value
        return values

    def extract_manytomany_values(self, values, model):
        """
        Extracts the ManyToMany values out of the values because they need to be
        created and updated in a different way compared to a regular value.

        :param values: The values where to extract the manytomany values from.
        :type values: dict
        :param model: The model containing the fields. The key, which is also the
            field name, is used to check in the model if the value is a ManyToMany
            value.
        :type model: Model
        :return: The values without the manytomany values and the manytomany values
            in another dict.
        :rtype: dict, dict
        """

        manytomany_values = {}

        for name, value in values.items():
            model_field = model._meta.get_field(name)
            if isinstance(model_field, ManyToManyField):
                manytomany_values[name] = values[name]

        for name in manytomany_values.keys():
            del values[name]

        return values, manytomany_values

    def get_unique_orders_before_row(
        self,
        before_row: GeneratedTableModel,
        model: Type[GeneratedTableModel],
        amount: int = 1,
    ) -> List[Decimal]:
        """
        Calculates a list of unique decimal orders that can safely be used before the
        provided `before_row`.

        :param before_row: The row instance where the before orders must be
            calculated for.
        :param model: The model of the related table
        :param amount: The number of orders that must be requested. Can be higher if
            multiple rows are inserted or moved.
        :return: A list of decimals containing safe to use orders in order.
        """

        if before_row:
            # In order to find the intermediate order, we need to figure out what the
            # order of the before adjacent row is. This queryset finds it in an
            # efficient way.
            adjacent_order = (
                model.objects.filter(order__lt=before_row.order)
                .aggregate(max=Max("order"))
                .get("max")
            ) or Decimal("0")
            new_orders = []
            new_order = adjacent_order
            for i in range(0, amount):
                float_order = find_intermediate_order(new_order, before_row.order)
                # Row orders "only" store 20 decimal places, so we're already
                # rounding it, so that the `order` will be set immediately.
                new_order = round(Decimal(float_order), 20)
                new_orders.append(new_order)
            return new_orders
        else:
            # If no `before_row` is provided, we can just find the highest value and
            # add one to it.
            step = Decimal("1.00000000000000000000")
            order_last_row = ceil(
                model.objects.aggregate(max=Max("order")).get("max") or Decimal("0")
            )
            return [order_last_row + (step * i) for i in range(1, amount + 1)]

    def get_row(
        self,
        user: AbstractUser,
        table: Table,
        row_id: int,
        model: Optional[Type[GeneratedTableModel]] = None,
        base_queryset: Optional[QuerySet] = None,
    ) -> GeneratedTableModel:
        """
        Fetches a single row from the provided table.

        :param user: The user of whose behalf the row is requested.
        :param table: The table where the row must be fetched from.
        :param row_id: The id of the row that must be fetched.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :param base_queryset: A queryset that can be used to already pre-filter
            the results.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        :return: The requested row instance.
        """

        if model is None:
            model = table.get_model()

        if base_queryset is None:
            base_queryset = model.objects

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            ReadDatabaseRowOperationType.type,
            group=group,
            context=table,
        )

        try:
            row = base_queryset.get(id=row_id)
        except model.DoesNotExist:
            raise RowDoesNotExist(row_id)

        return row

    def get_adjacent_row(self, row, original_queryset, previous=False, view=None):
        """
        Fetches the adjacent row of the provided row. By default, the next row will
        be fetched. This will be done by applying the order as greater than or lower
        than filter using the values of the provided row.

        :param row: An instance of the row where the adjacent row must be
            fetched from.
        :param original_queryset: The original queryset that was used to fetch the
            row. This should contain all the orders, annotations, filters that were
            used when fetching the row.
        :param previous: If the previous row should be fetched.
        :param view: The view which contains the sorts that need to be applied to the
            queryset, to find the right adjacent field.
        :return: The adjacent rows.
        """

        from baserow.contrib.database.views.handler import ViewHandler

        default_sorting = ["order", "id"]

        if previous:
            original_queryset = original_queryset.reverse()

        # Sort query set
        if view:
            queryset_sorted = ViewHandler().apply_sorting(view, original_queryset)
        else:
            direction_prefix = "-" if previous else ""
            sorting = [f"{direction_prefix}{sort}" for sort in default_sorting]
            queryset_sorted = original_queryset.order_by(*sorting)

        # Apply filters to find the adjacent row
        filter_builder = FilterBuilder(FILTER_TYPE_OR)

        previous_fields = {}
        # Append view sorting
        if view:
            for view_sort in view.viewsort_set.all():
                field = view_sort.field
                field_name = field.db_column
                field_type = field_type_registry.get_by_model(
                    view_sort.field.specific_class
                )

                if previous:
                    if view_sort.order == "DESC":
                        order_direction = "ASC"
                    else:
                        order_direction = "DESC"
                else:
                    order_direction = view_sort.order

                order_direction_suffix = "__gt" if order_direction == "ASC" else "__lt"

                order = field_type.get_order(field, field_name, order_direction)

                annotation = None
                expression_name = None
                if order is None:
                    # In this case there isn't a custom implementation for the order
                    # and we can assume that we can just filter by th field name.
                    filter_key = f"{field_name}{order_direction_suffix}"
                else:
                    # In this case the field type is more complex and probably requires
                    # joins in order to filter on the field. We will add the order
                    # expression to the queryset and filter on that expression.
                    annotation = order.annotation
                    order = order.order
                    expression_name = order.expression.name
                    filter_key = f"{expression_name}{order_direction_suffix}"

                value = field_type.get_value_for_filter(row, field_name)

                q_kwargs = copy(previous_fields)
                q_kwargs[filter_key] = value

                q = Q(**q_kwargs)

                # As the key we want to use the field name without any direction suffix.
                # In the case of a "normal" field type, that will just be the field_name
                # But in the case of a more complex field type, it might be the
                # expression name.
                # An expression name could look like `field_1__value` while a field name
                # will always be like `field_1`.
                previous_fields[expression_name or field_name] = value

                if annotation:
                    q = AnnotatedQ(annotation=annotation, q=q)

                filter_builder.filter(q)

        # Append default sorting
        for field_name in default_sorting:
            direction_suffix = "__lt" if previous else "__gt"
            filter_key = f"{field_name}{direction_suffix}"

            value = getattr(row, field_name)

            q_kwargs = copy(previous_fields)
            q_kwargs[filter_key] = value

            previous_fields[field_name] = value

            filter_builder.filter(Q(**q_kwargs))

        queryset_filtered = filter_builder.apply_to_queryset(queryset_sorted)

        return queryset_filtered.first()

    def get_row_for_update(
        self,
        user: AbstractUser,
        table: Table,
        row_id: int,
        enhance_by_fields: bool = False,
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> GeneratedTableModelForUpdate:
        """
        Fetches a single row from the provided table and lock it for update.

        :param user: The user of whose behalf the row is requested.
        :param table: The table where the row must be fetched from.
        :param row_id: The id of the row that must be fetched.
        :param enhance_by_fields: Enhances the queryset based on the
            `enhance_queryset` for each field in the table.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        :return: The requested row instance.
        """

        if model is None:
            model = table.get_model()

        base_queryset = model.objects.select_for_update(of=("self",))
        if enhance_by_fields:
            base_queryset = base_queryset.enhance_by_fields()

        row = self.get_row(
            user,
            table,
            row_id,
            model=model,
            base_queryset=base_queryset,
        )

        return cast(GeneratedTableModelForUpdate, row)

    def get_row_names(
        self, table: "Table", row_ids: List[int], model: "GeneratedTableModel" = None
    ) -> Dict[str, str]:
        """
        Returns the row names for all row ids specified in `row_ids` parameter from
        the given table.

        :param table: The table where the rows must be fetched from.
        :param row_ids: The id of the rows that must be fetched.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :return: A dict of the requested rows names. The key are the row ids and the
            values are the row names.
        """

        if not model:
            primary_field = table.field_set.get(primary=True)
            model = table.get_model(
                field_ids=[], fields=[primary_field], add_dependencies=False
            )

        queryset = model.objects.filter(pk__in=row_ids)

        return {row.id: str(row) for row in queryset}

    # noinspection PyMethodMayBeStatic
    def has_row(self, user, table, row_id, raise_error=False, model=None):
        """
        Checks if a row with the given id exists and is not trashed in the table.

        This method is preferred over using get_row when you do not actually need to
        access any values of the row as it will not construct a full model but instead
        do a much more efficient query to check only if the row exists or not.

        :param user: The user of whose behalf the row is being checked.
        :type user: User
        :param table: The table where the row must be checked in.
        :type table: Table
        :param row_id: The id of the row that must be checked.
        :type row_id: int
        :param raise_error: Whether or not to raise an Exception if the row does not
            exist or just return a boolean instead.
        :type raise_error: bool
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :type model: Model
        :raises RowDoesNotExist: When the row with the provided id does not exist
            and raise_error is set to True.
        :raises UserNotInGroup: If the user does not belong to the group.
        :return: If raise_error is False then a boolean indicating if the row does or
            does not exist.
        :rtype: bool
        """

        CoreHandler().check_permissions(
            user,
            ReadDatabaseRowOperationType.type,
            group=table.database.group,
            context=table,
        )

        if model is None:
            model = table.get_model(field_ids=[])

        row_exists = model.objects.filter(id=row_id).exists()
        if not row_exists and raise_error:
            raise RowDoesNotExist(row_id)
        else:
            return row_exists

    def create_row(
        self,
        user: AbstractUser,
        table: Table,
        values: Optional[Dict[str, Any]] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
        before_row: Optional[GeneratedTableModel] = None,
        user_field_names: bool = False,
    ) -> GeneratedTableModel:
        """
        Creates a new row for a given table with the provided values if the user
        belongs to the related group. It also calls the rows_created signal.

        :param user: The user of whose behalf the row is created.
        :param table: The table for which to create a row for.
        :param values: The values that must be set upon creating the row. The keys must
            be the field ids.
        :param model: If a model is already generated it can be provided here to avoid
            having to generate the model again.
        :param before_row: If provided the new row will be placed right before that row
            instance.
        :param user_field_names: Whether or not the values are keyed by the internal
            Baserow field name (field_1,field_2 etc) or by the user field names.
        :return: The created row instance.
        """

        if model is None:
            model = table.get_model()

        CoreHandler().check_permissions(
            user,
            CreateRowDatabaseTableOperationType.type,
            group=table.database.group,
            context=table,
        )

        instance = self.force_create_row(
            table, values, model, before_row, user_field_names
        )

        rows_created.send(
            self,
            rows=[instance],
            before=before_row,
            user=user,
            table=table,
            model=model,
        )

        return instance

    def force_create_row(
        self,
        table: Table,
        values: Optional[Dict[str, Any]] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
        before: Optional[GeneratedTableModel] = None,
        user_field_names: bool = False,
    ):
        """
        Creates a new row for a given table with the provided values.

        :param table: The table for which to create a row for.
        :param values: The values that must be set upon creating the row. The keys must
            be the field ids.
        :param model: If a model is already generated it can be provided here to avoid
            having to generate the model again.
        :param before: If provided the new row will be placed right before that row
            instance.
        :param user_field_names: Whether or not the values are keyed by the internal
            Baserow field name (field_1,field_2 etc) or by the user field names.
        :return: The created row instance.
        :rtype: Model
        """

        if not values:
            values = {}

        if model is None:
            model = table.get_model()

        if user_field_names:
            values = self.map_user_field_name_dict_to_internal(
                model._field_objects, values
            )

        values = self.prepare_values(model._field_objects, values)
        values, manytomany_values = self.extract_manytomany_values(values, model)
        values["order"] = self.get_unique_orders_before_row(before, model)[0]
        instance = model.objects.create(**values)

        for name, value in manytomany_values.items():
            getattr(instance, name).set(value)

        fields = []
        update_collector = FieldUpdateCollector(table, starting_row_ids=[instance.id])
        field_cache = FieldCache()
        field_ids = []
        for field_object in model._field_objects.values():
            field_type: FieldType = field_object["type"]
            field = field_object["field"]
            fields.append(field)
            field_ids.append(field.id)

            field_type.after_rows_created(
                field, [instance], update_collector, field_cache
            )

        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_created(
                dependant_field,
                instance,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        if model.fields_requiring_refresh_after_insert():
            instance.refresh_from_db(
                fields=model.fields_requiring_refresh_after_insert()
            )

        from baserow.contrib.database.views.handler import ViewHandler

        ViewHandler().field_value_updated(fields)

        return instance

    # noinspection PyMethodMayBeStatic
    def map_user_field_name_dict_to_internal(
        self,
        field_objects,
        values,
    ):
        """
        Takes the field objects for a model and a dictionary keyed by user specified
        field names for that model. Then will convert the keys from the user names to
        the internal Baserow field names which look like field_1, field_2 and
        correspond to the actual database column names.

        :param field_objects: The field objects for a model.
        :param values: A dictionary keyed by user field names to values.
        :return: A dictionary with the same values but the keys converted to the
            corresponding internal baserow field name (field_1,field_2 etc)
        """

        to_internal_name = {}
        for field_object in field_objects.values():
            to_internal_name[field_object["field"].name] = field_object["name"]

        mapped_back_to_internal_field_names = {}
        for user_field_name, value in values.items():
            internal_name = to_internal_name[user_field_name]
            mapped_back_to_internal_field_names[internal_name] = value
        values = mapped_back_to_internal_field_names
        return values

    def update_row_by_id(
        self,
        user: AbstractUser,
        table: Table,
        row_id: int,
        values: Dict[str, Any],
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> GeneratedTableModelForUpdate:
        """
        Updates one or more values of the provided row_id.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be updated.
        :param row_id: The id of the row that must be updated.
        :param values: The values that must be updated. The keys must be the field ids.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        :return: The updated row instance.
        """

        if model is None:
            model = table.get_model()

        with transaction.atomic():
            row = self.get_row_for_update(
                user, table, row_id, enhance_by_fields=True, model=model
            )
            return self.update_row(user, table, row, values, model=model)

    def update_row(
        self,
        user: AbstractUser,
        table: Table,
        row: GeneratedTableModelForUpdate,
        values: Dict[str, Any],
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> GeneratedTableModelForUpdate:
        """
        Updates one or more values of the provided row_id.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be updated.
        :param row: the row that must be updated.
        :param values: The values that must be updated. The keys must be the field ids.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        :return: The updated row instance.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user, UpdateDatabaseRowOperationType.type, group=group, context=table
        )

        if model is None:
            model = table.get_model()

        updated_fields_by_name = {}
        updated_fields = []
        updated_field_ids = set()
        for field_id, field in model._field_objects.items():
            if field_id in values or field["name"] in values:
                updated_field_ids.add(field_id)
                updated_fields_by_name[field["name"]] = field["field"]
                updated_fields.append(field["field"])

        before_return = before_rows_update.send(
            self,
            rows=[row],
            user=user,
            table=table,
            model=model,
            updated_field_ids=updated_field_ids,
        )

        values = self.prepare_values(model._field_objects, values)
        values, manytomany_values = self.extract_manytomany_values(values, model)

        for name, value in values.items():
            setattr(row, name, value)

        # This update can remove link row connections with other rows. We need to keep
        # track of these so we can later update any dependant cells in those rows that
        # we used to link to. This is a dictionary where the key is the id link row
        # field in this table, and the value is a set of row ids that this row used to
        # link to via that link row field.
        deleted_m2m_rels_per_link_field: Dict[int, Set[int]] = defaultdict(set)

        for name, value in manytomany_values.items():
            field = updated_fields_by_name[name]
            new_ids = set(value)
            # Uses the existing prefetch cache and so doesn't run queries.
            if isinstance(field, LinkRowField):
                for existing in getattr(row, name).all():
                    if existing.id not in new_ids:
                        deleted_m2m_rels_per_link_field[field.id].add(existing.id)
            getattr(row, name).set(value)

        row.save()

        update_collector = FieldUpdateCollector(
            table,
            starting_row_ids=[row.id],
            deleted_m2m_rels_per_link_field=deleted_m2m_rels_per_link_field,
        )
        field_cache = FieldCache()
        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            updated_field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_updated(
                dependant_field,
                row,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)
        # We need to refresh here as ExpressionFields might have had their values
        # updated. Django does not support UPDATE .... RETURNING and so we need to
        # query for the rows updated values instead.
        row.refresh_from_db(fields=model.fields_requiring_refresh_after_update())

        from baserow.contrib.database.views.handler import ViewHandler

        ViewHandler().field_value_updated(updated_fields)

        rows_updated.send(
            self,
            rows=[row],
            user=user,
            table=table,
            model=model,
            before_return=before_return,
            updated_field_ids=updated_field_ids,
        )

        return row

    def create_rows(
        self,
        user: AbstractUser,
        table: Table,
        rows_values: List[Dict[str, Any]],
        before_row: Optional[GeneratedTableModel] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
        send_signal=True,
        generate_error_report=False,
    ) -> List[GeneratedTableModel]:
        """
        Creates new rows for a given table if the user
        belongs to the related group. It also calls the rows_created signal.

        :param user: The user of whose behalf the rows are created.
        :param table: The table for which the rows should be created.
        :param rows_values: List of rows values for rows that need to be created.
        :param before_row: If provided the new rows will be placed right before
            the before_row.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :return: The created row instances.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            CreateRowDatabaseTableOperationType.type,
            group=group,
            context=table,
        )

        if model is None:
            model = table.get_model()

        unique_orders = self.get_unique_orders_before_row(
            before_row, model, amount=len(rows_values)
        )

        report = {}
        rows, errors = self.prepare_rows_in_bulk(
            model._field_objects,
            rows_values,
            generate_error_report=generate_error_report,
        )
        report.update({index: err for index, err in errors.items()})

        rows_relationships = []
        for index, row in enumerate(rows, start=-len(rows)):
            values, manytomany_values = self.extract_manytomany_values(row, model)
            values["order"] = unique_orders[index]
            instance = model(**values)

            relations = {
                field_name: value
                for field_name, value in manytomany_values.items()
                if value and len(value) > 0
            }
            rows_relationships.append((instance, relations))

        inserted_rows = model.objects.bulk_create(
            [row for (row, relations) in rows_relationships]
        )

        many_to_many = defaultdict(list)
        for index, row in enumerate(inserted_rows):
            _, manytomany_values = rows_relationships[index]
            for field_name, value in manytomany_values.items():
                through = getattr(model, field_name).through
                through_fields = through._meta.get_fields()
                value_column = None
                row_column = None

                model_field = model._meta.get_field(field_name)
                is_referencing_the_same_table = (
                    model_field.model == model_field.related_model
                )

                # Figure out which field in the many to many through table holds the row
                # value and which one contains the value.
                for field in through_fields:
                    if type(field) is not ForeignKey:
                        continue

                    if is_referencing_the_same_table:
                        # django creates 'from_tableXmodel' and 'to_tableXmodel'
                        # columns for self-referencing many_to_many relations.
                        row_column = field.get_attname_column()[1]
                        value_column = row_column.replace("from", "to")
                        break
                    elif field.remote_field.model == model:
                        row_column = field.get_attname_column()[1]
                    else:
                        value_column = field.get_attname_column()[1]

                for i in value:
                    many_to_many[field_name].append(
                        getattr(model, field_name).through(
                            **{
                                row_column: row.id,
                                value_column: i,
                            }
                        )
                    )

        for field_name, values in many_to_many.items():
            through = getattr(model, field_name).through
            through.objects.bulk_create(values)

        update_collector = FieldUpdateCollector(
            table, starting_row_ids=[row.id for row in inserted_rows]
        )
        field_cache = FieldCache()
        field_ids = []
        for field_object in model._field_objects.values():
            field_type = field_object["type"]
            field = field_object["field"]
            field_ids.append(field.id)

            field_type.after_rows_created(
                field, inserted_rows, update_collector, field_cache
            )

        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_created(
                dependant_field,
                inserted_rows,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        from baserow.contrib.database.views.handler import ViewHandler

        updated_fields = [o["field"] for o in model._field_objects.values()]
        ViewHandler().field_value_updated(updated_fields)

        if send_signal:
            rows_to_return = list(
                model.objects.all()
                .enhance_by_fields()
                .filter(id__in=[row.id for row in inserted_rows])
            )

            rows_created.send(
                self,
                rows=rows_to_return,
                before=before_row,
                user=user,
                table=table,
                model=model,
            )

        if generate_error_report:
            return inserted_rows, report
        return rows_to_return

    def validate_rows(
        self,
        table: Table,
        rows: List[Dict[str, Any]],
        progress: Optional[Progress] = None,
    ) -> Dict[str, Dict[str, Any]]:
        """
        Validates rows by batch and generates an error report.

        :param user: The user of whose behalf the rows are created.
        :param table: The table for which the rows should be created.
        :param rows: List of rows values for rows that need to be created.
        :param progress: Give a progress instance to track the progress of the import.
        :return: The error report.
        """

        from baserow.api.exceptions import RequestBodyValidationException
        from baserow.api.utils import validate_data
        from baserow.contrib.database.api.rows.serializers import (
            get_row_serializer_class,
        )

        if not rows:
            return {}

        if progress:
            progress.increment(state=ROW_IMPORT_VALIDATION)

        model = table.get_model()
        # Use serializer to validate incoming data
        validation_serializer = get_row_serializer_class(model)
        report = {}
        for count, chunk in enumerate(grouper(BATCH_SIZE, rows)):
            row_start_index = count * BATCH_SIZE
            try:
                validate_data(validation_serializer, list(chunk), many=True)
            except RequestBodyValidationException as e:
                for index, err in enumerate(e.detail["detail"]):
                    report[row_start_index + index] = err

            if progress:
                progress.increment(len(chunk))

        return report

    def create_rows_by_batch(
        self,
        user: AbstractUser,
        table: Table,
        rows: List[Dict[str, Any]],
        progress: Optional[Progress] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> Tuple[List[GeneratedTableModel], Dict[str, Dict[str, Any]]]:
        """
        Creates rows by batch and generates an error report instead of failing on first
        error.

        :param user: The user of whose behalf the rows are created.
        :param table: The table for which the rows should be created.
        :param rows: List of rows values for rows that need to be created.
        :param progress: Give a progress instance to track the progress of the import.
        :param model: Optional model to prevent recomputing table model.
        :return: The created rows and the error report.
        """

        if not rows:
            return [], {}

        if progress:
            progress.increment(state=ROW_IMPORT_CREATION)

        if model is None:
            model = table.get_model()

        report = {}
        all_created_rows = []
        for count, chunk in enumerate(grouper(BATCH_SIZE, rows)):
            row_start_index = count * BATCH_SIZE
            created_rows, creation_report = self.create_rows(
                user=user,
                table=table,
                model=model,
                rows_values=chunk,
                generate_error_report=True,
                send_signal=False,
            )

            for valid_index, field_errors in creation_report.items():
                report[int(valid_index) + row_start_index] = prepare_field_errors(
                    field_errors
                )

            if progress:
                progress.increment(len(chunk))

            all_created_rows += created_rows

        return all_created_rows, report

    def import_rows(
        self,
        user: AbstractUser,
        table: Table,
        data: List[List[Any]],
        validate: bool = True,
        progress: Optional[Progress] = None,
        send_signal: bool = True,
    ) -> Tuple[List[GeneratedTableModel], Dict[str, Dict[str, Any]]]:
        """
        Creates new rows for a given table if the user
        belongs to the related group. It also calls the rows_created if send_signal is
        `True`. The data are validated before the creation if validate is True.
        when a row fails to import, it doesn't stop the import. Instead an error report
        is created with the raised error for each field of each failing rows.

        :param user: The user of whose behalf the rows are created.
        :param table: The table for which the rows should be created.
        :param data: List of rows values for rows that need to be created.
        :param validate: If True the data are validated before the import.
        :param progress: Give a progress instance to track the progress of the import.
        :param send_signal: If True a row_created signal is send.

        :return: The created row instances and the error report.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user, ImportRowsDatabaseTableOperationType.type, group=group, context=table
        )

        error_report = RowErrorReport(data)

        model = table.get_model()

        fields = [
            field_object["field"]
            for field_object in model._field_objects.values()
            if not field_object["type"].read_only
        ]

        # Sort by order then by id
        fields.sort(key=lambda f: (f.order, f.id))

        for index, row in enumerate(data):
            # Check row length
            if len(row) > len(fields):
                error_report.add_error(
                    index,
                    {"non_field_errors": ["Too many values in this line."]},
                )
            else:
                new_row = list(row)
                # Fill incomplete rows with empty values
                new_row.extend([None] * (len(fields) - len(row)))

                # Reshape data by field as expected by the import
                error_report.update_row(
                    index,
                    {
                        f"field_{fields[index].id}": value
                        for index, value in enumerate(new_row)
                    },
                )

        # STEP 1: pre-validate data with serializer
        if validate:
            (
                valid_rows,
                original_row_index_mapping,
            ) = error_report.get_valid_rows_and_mapping()

            validation_sub_progress = (
                progress.create_child(50, len(valid_rows)) if progress else None
            )

            validation_report = self.validate_rows(
                table, valid_rows, progress=validation_sub_progress
            )

            for index, error in validation_report.items():
                error_report.add_error(original_row_index_mapping[int(index)], error)

        (
            valid_rows,
            original_row_index_mapping,
        ) = error_report.get_valid_rows_and_mapping()

        # STEP 2: create rows in DB
        creation_sub_progress = (
            progress.create_child(50 if validate else 100, len(valid_rows))
            if progress
            else None
        )

        created_rows, creation_report = self.create_rows_by_batch(
            user, table, valid_rows, progress=creation_sub_progress, model=model
        )

        # Add errors to global report
        for index, error in creation_report.items():
            error_report.add_error(
                original_row_index_mapping[int(index)],
                error,
            )

        if send_signal:
            rows_to_return = list(
                model.objects.all()
                .enhance_by_fields()
                .filter(id__in=[row.id for row in created_rows])
            )

            rows_created.send(
                self,
                rows=rows_to_return,
                before=None,
                user=user,
                table=table,
                model=model,
            )

        return created_rows, error_report.to_dict()

    def update_rows(
        self,
        user: AbstractUser,
        table: Table,
        rows: List,
        model: Optional[Type[GeneratedTableModel]] = None,
        rows_to_update: Optional[RowsForUpdate] = None,
    ) -> List[GeneratedTableModelForUpdate]:
        """
        Updates field values in batch based on provided rows with the new values.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be updated.
        :param rows: The list of rows with new values that should be set.
        :param model: If the correct model has already been generated it can be
            provided so that it does not have to be generated for a second time.
        :param rows_to_update: If the rows to update have already been generated
            it can be provided so that it does not have to be generated for a
            second time.
        :raises RowIdsNotUnique: When trying to update the same row multiple times.
        :raises RowDoesNotExist: When any of the rows don't exist.
        :return: The updated row instances.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            UpdateDatabaseRowOperationType.type,
            group=group,
            context=table,
        )

        if model is None:
            model = table.get_model()

        rows, _ = self.prepare_rows_in_bulk(model._field_objects, rows)
        row_ids = [row["id"] for row in rows]

        non_unique_ids = get_non_unique_values(row_ids)
        if len(non_unique_ids) > 0:
            raise RowIdsNotUnique(non_unique_ids)

        rows_by_id = {}
        for row in rows:
            row_id = row.pop("id")
            rows_by_id[row_id] = row

        if rows_to_update is None:
            rows_to_update = self.get_rows_for_update(model, row_ids)

        if len(rows_to_update) != len(rows):
            db_rows_ids = [db_row.id for db_row in rows_to_update]
            raise RowDoesNotExist(sorted(list(set(row_ids) - set(db_rows_ids))))

        updated_field_ids = set()
        field_name_to_field = dict()
        for obj in rows_to_update:
            row_values = rows_by_id[obj.id]
            for field_id, field in model._field_objects.items():
                field_name_to_field[field["name"]] = field["field"]
                if field_id in row_values or field["name"] in row_values:
                    updated_field_ids.add(field_id)

        before_return = before_rows_update.send(
            self,
            rows=list(rows_to_update),
            user=user,
            table=table,
            model=model,
            updated_field_ids=updated_field_ids,
        )

        rows_relationships = []
        for obj in rows_to_update:
            # The `updated_on` field is not updated with `bulk_update`,
            # so we manually set the value here.
            obj.updated_on = model._meta.get_field("updated_on").pre_save(
                obj, add=False
            )
            row_values = rows_by_id[obj.id]
            values, manytomany_values = self.extract_manytomany_values(
                row_values, model
            )

            for name, value in values.items():
                setattr(obj, name, value)

            relations = {
                field_name: value
                for field_name, value in manytomany_values.items()
                if value or isinstance(value, list)
            }
            rows_relationships.append(relations)

            fields_with_pre_save = model.fields_requiring_refresh_after_update()
            for field_name in fields_with_pre_save:
                setattr(
                    obj,
                    field_name,
                    model._meta.get_field(field_name).pre_save(obj, add=False),
                )

        many_to_many = defaultdict(list)
        row_column_name = None
        row_ids_change_m2m_per_field = defaultdict(set)

        # This update can remove link row connections with other rows. We need to keep
        # track of these so we can later update any dependant cells in those rows that
        # we used to link to. This is a dictionary where the key is the id link row
        # field in this table, and the value is a set of row ids that these rows used to
        # link to via that link row field.
        deleted_m2m_rels_per_link_field: Dict[int, Set[int]] = defaultdict(set)

        for index, row in enumerate(rows_to_update):
            manytomany_values = rows_relationships[index]
            for field_name, value in manytomany_values.items():
                through = getattr(model, field_name).through
                through_fields = through._meta.get_fields()
                value_column = None
                row_column = None

                model_field = model._meta.get_field(field_name)
                is_referencing_the_same_table = (
                    model_field.model == model_field.related_model
                )

                # Figure out which field in the many to many through table holds the row
                # value and which one contains the value.
                for field in through_fields:
                    if type(field) is not ForeignKey:
                        continue

                    row_ids_change_m2m_per_field[field_name].add(row.id)

                    if is_referencing_the_same_table:
                        # django creates 'from_tableXmodel' and 'to_tableXmodel'
                        # columns for self-referencing many_to_many relations.
                        row_column = field.get_attname_column()[1]
                        row_column_name = row_column
                        value_column = row_column.replace("from", "to")
                        break
                    elif field.remote_field.model == model:
                        row_column = field.get_attname_column()[1]
                        row_column_name = row_column
                    else:
                        value_column = field.get_attname_column()[1]

                # If this m2m field is a link row we need to find out all connections
                # which will be removed by this update. This is so we can update
                # rows which previously were connected to an updated row, but no
                # longer are.
                field = field_name_to_field[field_name]
                if isinstance(field, LinkRowField):
                    # Uses the existing prefetch cache and so doesn't run queries.
                    m2m_rels_before_update = {
                        r.id for r in getattr(row, field_name).all()
                    }
                    deleted_m2m_rels_per_link_field[field.id].update(
                        m2m_rels_before_update
                    )

                if len(value) == 0:
                    many_to_many[field_name].append(None)
                else:
                    for i in value:
                        # After we have discarded all connections the user has provided
                        # we will be left with only the deleted connections as desired.
                        deleted_m2m_rels_per_link_field[field.id].discard(i)
                        many_to_many[field_name].append(
                            getattr(model, field_name).through(
                                **{
                                    row_column: row.id,
                                    value_column: i,
                                }
                            )
                        )

        # The many to many relations need to be updated first because they need to
        # exist when the rows are updated in bulk. Otherwise, the formula and lookup
        # fields can't see the relations.
        for field_name, values in many_to_many.items():
            through = getattr(model, field_name).through
            filter = {
                f"{row_column_name}__in": row_ids_change_m2m_per_field[field_name]
            }
            delete_qs = through.objects.all().filter(**filter)
            delete_qs._raw_delete(delete_qs.db)
            through.objects.bulk_create([v for v in values if v is not None])

        bulk_update_fields = ["updated_on"]
        for field in model._field_objects.values():
            field_name = field["name"]
            model_field = model._meta.get_field(field_name)
            # For now all valid fields that don't represent a relationship will be
            # used in the bulk_update() call. This could be optimized in the future
            # if we can select just fields that need to be updated (fields that are
            # passed in + read only fields that need updating too)
            not_m2m = not isinstance(model_field, ManyToManyField)
            if not_m2m and getattr(model_field, "valid_for_bulk_update", True):
                bulk_update_fields.append(field_name)

        if len(bulk_update_fields) > 0:
            model.objects.bulk_update(rows_to_update, bulk_update_fields)

        update_collector = FieldUpdateCollector(
            table,
            starting_row_ids=row_ids,
            deleted_m2m_rels_per_link_field=deleted_m2m_rels_per_link_field,
        )
        field_cache = FieldCache()
        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            updated_field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_updated(
                dependant_field,
                rows_to_update,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        from baserow.contrib.database.views.handler import ViewHandler

        updated_fields = [o["field"] for o in model._field_objects.values()]
        ViewHandler().field_value_updated(updated_fields)

        rows_to_return = list(
            model.objects.all().enhance_by_fields().filter(id__in=row_ids)
        )
        rows_updated.send(
            self,
            rows=rows_to_return,
            user=user,
            table=table,
            model=model,
            before_return=before_return,
            updated_field_ids=updated_field_ids,
        )

        return rows_to_return

    def get_rows_for_update(
        self, model: GeneratedTableModel, row_ids: List[int]
    ) -> RowsForUpdate:
        """
        Get the rows to update.
        """

        return cast(
            RowsForUpdate,
            model.objects.select_for_update(of=("self",))
            .enhance_by_fields()
            .filter(id__in=row_ids),
        )

    def move_row_by_id(
        self,
        user: AbstractUser,
        table: Table,
        row_id: int,
        before_row: Optional[GeneratedTableModel] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> GeneratedTableModelForUpdate:
        """
        Updates the row order value.

        :param user: The user of whose behalf the row is moved
        :param table: The table that contains the row that needs to be moved.
        :param row_id: The row id that needs to be moved.
        :param before_row: If provided the new row will be placed right before that row
            instance. Otherwise the row will be moved to the end.
        :param model: If the correct model has already been generated, it can be
            provided so that it does not have to be generated for a second time.
        """

        if model is None:
            model = table.get_model()

        with transaction.atomic():
            row = self.get_row_for_update(user, table, row_id, model=model)
            return self.move_row(user, table, row, before_row=before_row, model=model)

    def move_row(
        self,
        user: AbstractUser,
        table: Table,
        row: GeneratedTableModelForUpdate,
        before_row: Optional[GeneratedTableModel] = None,
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> GeneratedTableModelForUpdate:
        """
        Updates the row order value.

        :param user: The user of whose behalf the row is moved
        :param table: The table that contains the row that needs to be moved.
        :param row: The row that needs to be moved.
        :param before_row: If provided the new row will be placed right before that row
            instance. Otherwise the row will be moved to the end.
        :param model: If the correct model has already been generated, it can be
            provided so that it does not have to be generated for a second time.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            MoveRowDatabaseRowOperationType.type,
            group=group,
            context=table,
        )

        if model is None:
            model = table.get_model()

        before_return = before_rows_update.send(
            self, rows=[row], user=user, table=table, model=model, updated_field_ids=[]
        )

        row.order = self.get_unique_orders_before_row(before_row, model)[0]
        row.save()

        update_collector = FieldUpdateCollector(table, starting_row_ids=[row.id])
        field_cache = FieldCache()
        updated_field_ids = []
        updated_fields = []
        for field_id, field_object in model._field_objects.items():
            updated_field_ids.append(field_id)
            field = field_object["field"]
            updated_fields.append(field)

        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            updated_field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_moved(
                dependant_field,
                row,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        from baserow.contrib.database.views.handler import ViewHandler

        ViewHandler().field_value_updated(updated_fields)

        rows_updated.send(
            self,
            rows=[row],
            user=user,
            table=table,
            model=model,
            before_return=before_return,
            updated_field_ids=[],
        )

        return row

    def delete_row_by_id(
        self,
        user: AbstractUser,
        table: Table,
        row_id: int,
        model: Optional[Type[GeneratedTableModel]] = None,
    ):
        """
        Deletes an existing row of the given table and with row_id.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be deleted.
        :param row_id: The id of the row that must be deleted.
        :param model: If the correct model has already been generated, it can be
            provided so that it does not have to be generated for a second time.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        """

        if model is None:
            model = table.get_model()

        with transaction.atomic():
            row = self.get_row(user, table, row_id, model=model)
            self.delete_row(user, table, row, model=model)

    def delete_row(
        self,
        user: AbstractUser,
        table: Table,
        row: GeneratedTableModelForUpdate,
        model: Optional[Type[GeneratedTableModel]] = None,
    ):
        """
        Deletes an existing row of the given table and with row_id.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be deleted.
        :param row: The row that must be deleted.
        :param model: If the correct model has already been generated, it can be
            provided so that it does not have to be generated for a second time.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            DeleteDatabaseRowOperationType.type,
            group=group,
            context=table,
        )

        if model is None:
            model = table.get_model()

        before_return = before_rows_delete.send(
            self, rows=[row], user=user, table=table, model=model
        )

        TrashHandler.trash(user, group, table.database, row, parent_id=table.id)

        update_collector = FieldUpdateCollector(table, starting_row_ids=[row.id])
        field_cache = FieldCache()
        updated_field_ids = []
        updated_fields = []

        for field_id, field_object in model._field_objects.items():
            updated_field_ids.append(field_id)
            field = field_object["field"]
            updated_fields.append(field)

        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            updated_field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_deleted(
                dependant_field,
                row,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        from baserow.contrib.database.views.handler import ViewHandler

        ViewHandler().field_value_updated(updated_fields)

        rows_deleted.send(
            self,
            rows=[row],
            user=user,
            table=table,
            model=model,
            before_return=before_return,
        )

    def delete_rows(
        self,
        user: AbstractUser,
        table: Table,
        row_ids: List[int],
        model: Optional[Type[GeneratedTableModel]] = None,
    ) -> TrashedRows:
        """
        Trashes existing rows of the given table based on row_ids.

        :param user: The user of whose behalf the change is made.
        :param table: The table for which the row must be deleted.
        :param row_ids: The ids of the rows that must be deleted.
        :param model:
        :param model: If the correct model has already been generated, it can be
            provided so that it does not have to be generated for a second time.
        :raises RowDoesNotExist: When the row with the provided id does not exist.
        """

        group = table.database.group
        CoreHandler().check_permissions(
            user,
            DeleteDatabaseRowOperationType.type,
            group=group,
            context=table,
        )

        if not model:
            model = table.get_model()

        non_unique_ids = get_non_unique_values(row_ids)
        if len(non_unique_ids) > 0:
            raise RowIdsNotUnique(non_unique_ids)

        rows = list(model.objects.filter(id__in=row_ids).enhance_by_fields())

        if len(row_ids) != len(rows):
            db_rows_ids = [db_row.id for db_row in rows]
            raise RowDoesNotExist(sorted(list(set(row_ids) - set(db_rows_ids))))

        before_return = before_rows_delete.send(
            self, rows=rows, user=user, table=table, model=model
        )

        trashed_rows = TrashedRows.objects.create(row_ids=row_ids, table=table)
        # It's a bit on a hack, but we're storing the fetched row objects on the
        # trashed_rows object, so that they can optionally be used later. This is for
        # example used when storing the names in the trash.
        trashed_rows.rows = rows

        TrashHandler.trash(
            user, group, table.database, trashed_rows, parent_id=table.id
        )

        updated_field_ids = []
        updated_fields = []
        for field_id, field_object in model._field_objects.items():
            updated_field_ids.append(field_id)
            field = field_object["field"]
            updated_fields.append(field)

        update_collector = FieldUpdateCollector(table, starting_row_ids=row_ids)
        field_cache = FieldCache()
        for (
            dependant_field,
            dependant_field_type,
            path_to_starting_table,
        ) in FieldDependencyHandler.get_dependant_fields_with_type(
            table.id,
            updated_field_ids,
            associated_relations_changed=True,
            field_cache=field_cache,
        ):
            dependant_field_type.row_of_dependency_deleted(
                dependant_field,
                rows,
                update_collector,
                field_cache,
                path_to_starting_table,
            )
        update_collector.apply_updates_and_get_updated_fields(field_cache)

        from baserow.contrib.database.views.handler import ViewHandler

        ViewHandler().field_value_updated(updated_fields)

        rows_deleted.send(
            self,
            rows=rows,
            user=user,
            table=table,
            model=model,
            before_return=before_return,
        )

        return trashed_rows
